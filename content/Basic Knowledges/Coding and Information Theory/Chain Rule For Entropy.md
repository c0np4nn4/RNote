여기서는 확률 변수들에 대한 엔트로피([[Entropy]])의 모음이 조건부 엔트로피([[Conditional Entropy]])의 합과 같음을 보입니다.

## Theorem
$p(x_1, x_2, \dots, x_n)$ 의 분포를 따르는 확률변수 $X_1, X_2, \dots, X_n$ 에 대하여 아래 식이 성립합니다.

$$
H(X_1, X_2, \dots, X_n) = \sum_{i=1}^n H(X_i | X_i-1, \dots, X_1)
$$
## Proof

[[Chain Rule]]에 따라 확률 변수 $X_1, X_2$에 대한 결합 엔트로피([[Joint Entropy]])를 정리해보면 아래와 같습니다.
$$
H(X_1, X_2) = H(X_1) + H(X_2|X_1)
$$
이번에는 $X_2, X_3$을 하나로 보고 아래와 같이 또 다른 체인룰을 생각해볼 수 있습니다.
$$
H(X_1, X_2, X_3) = H(X_1) + H(X_2, X_3|X_1)
$$
이 때, $H(X_2, X_3|X_1)$ 에서 $X_1$는 '주어진 정보' 또는 '이미 알고 있는 지식' 이므로, 잠시 가려두고 $H(X_2, X_3)$ 처럼 생각하여 체인룰에 따라 조건부 엔트로피의 형태로 식을 정리할 수 있습니다.
$$
\begin{aligned}
H(X_1, X_2, X_3) &= H(X_1) + H(X_2, X_3 | X_1) \\
&=H(X_1) + H(X_2 | X_1) + H(X_3 | X_2, X_1)
\end{aligned}
$$
이는 [[Chain Rule]]에서의 Corollary를 따른 것이기도 합니다.

이런 방식으로 체인룰을 활용해 $n$개의 확률 변수에 대한 결합 엔트로피를 정리하면 아래와 같습니다.

$$
\begin{aligned}
H(X_1, X_2, \dots, X_n) &= H(X_1) + H(X_2|X_1) + H(X_3| X_2, X_1) + \cdots + H(X_n | X_{n-1}, \cdots, X_1) \\
&= \sum_{i=1}^n H(X_i | X_{i-1}, \dots, X_1)
\end{aligned}
$$

---

즉, 아래와 같은 벤 다이어그램으로 [[Entropy]]에 대해 생각해볼 수 있습니다.
![[Pasted image 20250321194257.png|500]]

위 그림은 각각의 확률 변수에 대한 불확실성([[Uncertainty]])이 앞선 확률변수에 대한 정보가 주어졌을 때 점점 줄어들면서 더해짐을 시각적으로 나타냅니다. 즉, $H(X, Y, Z) = H(X) + H(Y|X) + H(Z|X, Y)$ 를 의미합니다.